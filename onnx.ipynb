{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX (open neural network exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ONNX became widely recognized as a standardized format that facilitates the representation of deep learning models.\n",
    "* Ability to promote seamless interchange and collaboration between various frameworks.\n",
    "\n",
    "* check and ensure the following four aspects for a successful conversion with ONNX-\n",
    "1) Model traiing\n",
    "2) Input and output names\n",
    "3) Handling Dynamic Axes - allowing tensors to represent parameters like batch size or sequence length.\n",
    "4) Conversion Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Framework not specified. Using pt to export the model.\n",
    "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
    "```\n",
    "pip install accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onnxruntime (ORT)- (dedicated accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q onnx optimum diffusers tf-keras accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining PyTorch model\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = torch.nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Creating an instance\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input example\n",
    "example_input = torch.randn(1, 10)\n",
    "\n",
    "# Exporting to ONNX format\n",
    "torch.onnx.export(model, example_input, \"linear.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"linear.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ir_version: 8\n",
       "opset_import {\n",
       "  version: 17\n",
       "}\n",
       "producer_name: \"pytorch\"\n",
       "producer_version: \"2.2.1\"\n",
       "graph {\n",
       "  node {\n",
       "    input: \"onnx::Gemm_0\"\n",
       "    input: \"fc.weight\"\n",
       "    input: \"fc.bias\"\n",
       "    output: \"3\"\n",
       "    name: \"/fc/Gemm\"\n",
       "    op_type: \"Gemm\"\n",
       "    attribute {\n",
       "      name: \"alpha\"\n",
       "      type: FLOAT\n",
       "      f: 1\n",
       "    }\n",
       "    attribute {\n",
       "      name: \"beta\"\n",
       "      type: FLOAT\n",
       "      f: 1\n",
       "    }\n",
       "    attribute {\n",
       "      name: \"transB\"\n",
       "      type: INT\n",
       "      i: 1\n",
       "    }\n",
       "  }\n",
       "  name: \"main_graph\"\n",
       "  initializer {\n",
       "    dims: 10\n",
       "    dims: 10\n",
       "    data_type: 1\n",
       "    name: \"fc.weight\"\n",
       "    raw_data: \"\\214N\\216\\276\\226\\377\\372\\273\\306\\274R<\\004\\031\\240\\275\\256\\177 \\276+\\207\\001\\276x\\026\\212\\272\\014[\\260=j\\266\\215\\276\\234|-<4\\305\\214\\276\\253\\026L>\\222\\303\\213\\276\\344;(=\\016\\025Z\\276\\367\\001i\\275\\312\\322j>\\005\\235\\003\\276\\213$\\034\\276\\203P\\323\\275\\364\\\\\\236\\276\\351q)=\\354\\272e\\276{\\321$<.\\344\\016\\276\\251{:>\\271g1=\\333\\375\\213\\274\\250G\\224>\\373m\\201\\276\\201N\\313\\275+\\357s\\276\\276\\223?\\276e\\030W>(x\\343\\275\\265\\227\\327\\274\\327IH\\275\\231\\301\\017\\275G]Y\\276\\026\\032,>P\\223\\340\\275\\357bz\\276\\361\\266\\020\\276^\\314\\224>\\014\\tw\\275\\305]\\007\\276\\375*^\\275\\212\\215\\226>\\215x\\206>\\020\\316K>h\\314b\\276q\\375\\030\\275\\250\\016\\363\\275\\013\\344\\203\\276\\347\\320\\035\\275\\\"\\255\\223\\276\\000m\\356\\275\\271\\252a>\\232\\351\\334\\275[6\\223\\276\\373\\201\\201\\2762\\030\\325\\275\\027$,\\276\\213n\\362<\\024\\020N\\276]\\310\\372\\275\\3225P>\\337Dm\\275\\206t\\275=\\371iv>W[\\217<6\\\"\\224=\\331\\016\\030\\275k\\321\\235<\\352\\016+\\276\\000\\260\\021>\\355\\347p>k\\034\\215=,\\263\\311\\274\\236Qk>\\374\\234I>,so>\\\"\\036\\240\\276a\\000\\222>\\216\\007\\r>\\262\\337\\230>\\2414\\010>\\377\\205\\300=\\231\\361F>\\216\\363[\\275\\350?a\\276\\017m\\215\\276\\257\\030\\376;E\\230d\\276\\343$\\232\\276\\312cs\\276\\344\\017\\221=L\\253n>\\315\\233\\240>\\177\\305\\355<\"\n",
       "  }\n",
       "  initializer {\n",
       "    dims: 10\n",
       "    data_type: 1\n",
       "    name: \"fc.bias\"\n",
       "    raw_data: \"\\232OS>\\245\\255\\252=E\\037 >\\177\\264\\235\\276\\335x\\035\\276\\376\\203\\237\\275\\316\\271[\\275K\\324\\224>O\\361L=\\236\\370\\377=\"\n",
       "  }\n",
       "  input {\n",
       "    name: \"onnx::Gemm_0\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "          dim {\n",
       "            dim_value: 10\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  output {\n",
       "    name: \"3\"\n",
       "    type {\n",
       "      tensor_type {\n",
       "        elem_type: 1\n",
       "        shape {\n",
       "          dim {\n",
       "            dim_value: 1\n",
       "          }\n",
       "          dim {\n",
       "            dim_value: 10\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "# Compare the output of the original model and the ONNX-converted model to ensure their equivalence.\n",
    "original_output = model(example_input)\n",
    "\n",
    "onnx_model = onnx.load(\"linear.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "rep = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "\n",
    "# onnx.checker.check_shapes(rep)\n",
    "rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Output: tensor([[ 0.1367, -0.1534, -0.6783,  0.0909, -0.0157, -0.8290, -0.1983,  0.6777,\n",
      "          0.4129, -0.6922]], grad_fn=<AddmmBackward0>)\n",
      "Onnx model Output: [[ 0.13670191 -0.15336278 -0.6782581   0.09087509 -0.01571308 -0.8289522\n",
      "  -0.1983419   0.67769706  0.41286647 -0.6922476 ]]\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnx_model.SerializeToString())\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: example_input.numpy()} # send sample input as dictionary\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "np.testing.assert_allclose(original_output.detach().numpy(), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "print(\"Original Output:\", original_output)\n",
    "print(\"Onnx model Output:\", ort_outs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart Examples for PyTorch, TensorFlow, and SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model using torch.onnx.export\n",
    "\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.onnx.export(model,                                # model being run\n",
    "                  torch.randn(1, 10, 10).to(device),    # model input (or a tuple for multiple inputs)\n",
    "                  \"fashion_model.onnx\",           # where to save the model (can be a file or file-like object)\n",
    "                  input_names = ['input'],              # the model's input names\n",
    "                  output_names = ['output'])            # the model's output names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"fashion_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference session using ort.InferenceSession\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "ort_sess = ort.InferenceSession('fashion_model.onnx')\n",
    "outputs = ort_sess.run(None, {'input': x.numpy()})\n",
    "\n",
    "# Print Result\n",
    "predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another sample\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                (text, offsets),           # model input (or a tuple for multiple inputs)\n",
    "                \"ag_news_model.onnx\",      # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=10,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['input', 'offsets'],   # the model's input names\n",
    "                output_names = ['output'], # the model's output names\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                              'output' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onnxruntime in optimum huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTStableDiffusionPipeline # Needs diffusers along with optimum, and accelerator for less cpu space requirement\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = ORTStableDiffusionPipeline.from_pretrained(model_id, export=True)\n",
    "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
    "image = pipeline(prompt).images[0]\n",
    "pipeline.save_pretrained(\"./onnx-stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum.onnxruntime.ORTModelForXXX model classes are API compatible with Hugging Face Transformers models. This means you can just replace your AutoModelForXXX class with the corresponding ORTModelForXXX class in optimum.onnxruntime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "-from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "+from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "-model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\") # PyTorch checkpoint\n",
    "\n",
    "+model = ORTModelForQuestionAnswering.from_pretrained(\"optimum/roberta-base-squad2\") # ONNX checkpoint\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "onnx_qa = pipeline(\"question-answering\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "question = \"What's my name?\"\n",
    "\n",
    "context = \"My name is Philipp and I live in Nuremberg.\"\n",
    "\n",
    "pred = onnx_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
