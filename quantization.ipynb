{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "* Quantization is a model optimization technique that aims to reduce the model's size and speed up the inference process from the models by simplifying the mathematical operations the model performs to reach an output value using an input value.\n",
    "\n",
    "\n",
    "* In deep learning, quantization refers to the process of reducing the precision of the weights and activations of a neural network model. \n",
    "* Reduced memory footprint\n",
    "* Faster inference\n",
    "* Energy efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Larger models are able to  maintain their capacities even when converted to 4-bit, with some techniques such as the NF4 suggesting no impact on their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = LSTM+Linear\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "       model,\n",
    "      {nn.LSTM, nn.Linear}, # determines the set of layers to dynamically quantize\n",
    "      dtype=torch.qint8 # determines the target dtype for quantized weights\n",
    ")\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "\n",
    "# # Due to using GPTQ\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE THREE MODES OF QUANTIZATION\n",
    "1. **Dynamic Quantization**\n",
    "* This involves not just converting the weights to int8 but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). \n",
    "* The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. \n",
    "* However, the activations are read and written to memory in floating point format.\n",
    "\n",
    "2. **Post-Training Static Quantization**\n",
    "* improve the performance (latency) by converting networks to use both integer arithmetic and int8 memory accesses. \n",
    "* Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations.\n",
    "* This information is used to determine how specifically the different activations should be quantized at inference time. \n",
    "* Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
    "\n",
    "3. **Quantization Aware Training**  \n",
    "* This typically results in highest accuracy of these three. With QAT, \n",
    "* all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers.\n",
    "* Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use already quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to easily quantize a model using AutoGPTQ along with the Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF4 and Double Quantization can be leveraged using the bitsandbytes library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suports - `load_in_8bit` or `load_in_4bit`\n",
    "* Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True, # Also supports load_in_8bit\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True, # double quantization\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16 # Specifies computational dtype which can be different than input type\n",
    ")\n",
    "\n",
    "model_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "\n",
    "tokenizer_nf4 = AutoTokenizer.from_pretrained(model_name, quantization_config=nf4_config)\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize pytorch model with Accelerate and `bitsandbytes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mingpt.model import GPT\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import load_and_quantize_model, BnbQuantizationConfig\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization:\n",
    "* This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from bitsandbytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "prompt = \"Hello my name is\"\n",
    "tokenizer = BPETokenizer()\n",
    "x1 = tokenizer(prompt).to(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've stored the weights of the GPT2 model in huggingface hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download weights from huggingface hub\n",
    "weights_location = snapshot_download(repo_id=\"marcsun13/gpt2-xl-linear-sharded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading 8-bit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model under the init_empty_weights context manager in order to load an empty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2-xl'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 1024\n",
    "\n",
    "# load model on meta device\n",
    "with init_empty_weights():\n",
    "  empty_model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModuleDict(\n",
      "          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(empty_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the quantization configuration that we want and call `load_and_quantize_model` to load the weights and quantize our empty GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantization config\n",
    "config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold=6)\n",
    "model_8bit = load_and_quantize_model(empty_model,\n",
    "                                     bnb_quantization_config = config,\n",
    "                                     weights_location = weights_location,\n",
    "                                     device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `nn.Linear` layers are replaced by `bnb.nn.Linear8bitLt` layers. `lm_head` was not replaced in order to ensure stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear8bitLt(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): Linear8bitLt(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModuleDict(\n",
      "          (c_fc): Linear8bitLt(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): Linear8bitLt(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_8bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the model in evaluation mode and we run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Doe, and I am a member of the\n"
     ]
    }
   ],
   "source": [
    "model_8bit.eval()\n",
    "outputs = model_8bit.generate(x1, max_new_tokens=10, do_sample=False)[0]\n",
    "print(tokenizer.decode(outputs.cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving 8-bit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save 8-bit model with `save_model` method from `Accelerator()`. Then, you can use these new weights to load your 8-bit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate = Accelerator()\n",
    "new_weights_location = \"gpt2-xl-linear-8-bit\"\n",
    "accelerate.save_model(model_8bit, new_weights_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading 4-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt2-xl'\n",
    "model_config.vocab_size = 50257\n",
    "model_config.block_size = 1024\n",
    "\n",
    "with init_empty_weights():\n",
    "  empty_model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantization config\n",
    "config = BnbQuantizationConfig(load_in_4bit=True,\n",
    "                               bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                               bnb_4bit_use_double_quant=True, # Quantize the already quantized values \n",
    "                               bnb_4bit_quant_type=\"nf4\" # This sets the quantization data type in the bnb.nn.Linear4Bit layers. \n",
    "                               # Options are FP4 and NF4 data types \n",
    "                               )\n",
    "\n",
    "model_4bit = load_and_quantize_model(empty_model,\n",
    "                                     bnb_quantization_config = config,\n",
    "                                     weights_location = weights_location,\n",
    "                                     device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `nn.Linear` layers are replaced by `bnb.nn.Linear4bit layers`. `lm_head` was not replaced in order to ensure stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear4bit(in_features=1600, out_features=4800, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=1600, out_features=1600, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModuleDict(\n",
      "          (c_fc): Linear4bit(in_features=1600, out_features=6400, bias=True)\n",
      "          (c_proj): Linear4bit(in_features=6400, out_features=1600, bias=True)\n",
      "          (act): NewGELU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Doe, I am a student at the University\n"
     ]
    }
   ],
   "source": [
    "model_4bit.eval()\n",
    "outputs = model_4bit.generate(x1, max_new_tokens=10, do_sample=False)[0]\n",
    "print(tokenizer.decode(outputs.cpu().squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitsandbytes\n",
    "* 4bit quantization such as NF4 (normalized float 4 (default)) or pure FP4 quantization. \n",
    "* While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit \n",
    "* This will enable a second quantization after the first one to save an additional 0.4 bits per parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\", # use NF4 for higher precision\n",
    "   bnb_4bit_use_double_quant=True, # use double quant if you have problems with memory\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16 # use a 16-bit dtype for faster finetuning\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
