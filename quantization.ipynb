{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "* Quantization is a model optimization technique that aims to reduce the model's size and speed up the inference process from the models by simplifying the mathematical operations the model performs to reach an output value using an input value.\n",
    "\n",
    "\n",
    "* In deep learning, quantization refers to the process of reducing the precision of the weights and activations of a neural network model. \n",
    "* Reduced memory footrint\n",
    "* Faster inference\n",
    "* Energy efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Larger models are able to  maintain their capacities even when converted to 4-bit, with some techniques such as the NF4 suggesting no impact on their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = LSTM+Linear\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "       model,\n",
    "      {nn.LSTM, nn.Linear}, # determines the set of layers to dynamically quantize\n",
    "      dtype=torch.qint8 # determines the target dtype for quantized weights\n",
    ")\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "\n",
    "# # Due to using GPTQ\n",
    "# !pip install optimum\n",
    "# !pip install auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use already quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to easily quantize a model using AutoGPTQ along with the Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NF4 and Double Quantization can be leveraged using the bitsandbytes library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suports - `load_in_8bit` or `load_in_4bit`\n",
    "* Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True, # Also supports load_in_8bit\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True, # double quantization\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16 # Specifies computational dtype which can be different than input type\n",
    ")\n",
    "\n",
    "model_name = \"PY007/TinyLlama-1.1B-step-50K-105b\"\n",
    "\n",
    "tokenizer_nf4 = AutoTokenizer.from_pretrained(model_name, quantization_config=nf4_config)\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
