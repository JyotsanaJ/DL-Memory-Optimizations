{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT (Parameter Efficient Fine Tuning):\n",
    "* A library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly\n",
    "* Adapters were one of the first parameter-efficient fine-tuning techniques released. showed that you can add more layers to the pre-existing transformer architecture and only finetune them instead of the whole model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q peft ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PromptTuningConfig\n",
    "import torch\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported PEFT types:\n",
    "    - PROMPT_TUNING\n",
    "    - MULTITASK_PROMPT_TUNING\n",
    "    - P_TUNING\n",
    "    - PREFIX_TUNING\n",
    "    - LORA (Default value)\n",
    "    - ADALORA\n",
    "    - ADAPTION_PROMPT\n",
    "    - IA3\n",
    "    - LOHA\n",
    "    - LOKR\n",
    "    - OFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the supported task types:\n",
    "    - SEQ_CLS: Text classification.\n",
    "    - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n",
    "    - CAUSAL_LM: Causal language modeling.\n",
    "    - TOKEN_CLS: Token classification.\n",
    "    - QUESTION_ANS: Question answering.\n",
    "    - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n",
    "      for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.SEQ_2_SEQ_LM: 'SEQ_2_SEQ_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"r : the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    bias: Specifies if the bias parameters should be trained.\n",
    "\"\"\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, # \n",
    "    target_modules=None, # The names of the modules to apply the adapter to. \n",
    "    inference_mode=False, # whether you’re using the model for inference or not\n",
    "    r=8,  # Lora attention dimension (the \"rank\")\n",
    "    lora_alpha=32, # The alpha parameter for Lora scaling for Low Ranking matrices.\n",
    "    lora_dropout=0.1 #The dropout probability for Lora layers.\n",
    ")\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path) ## 4.92GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.19151053100118282\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will only save the incremental PEFT weights that were trained.\n",
    "## safetensors saved which is 9MB\n",
    "\n",
    "model.save_pretrained(\"model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = \"model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Merge base model with peft saved weights\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honda CUST SERVICE\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"mps\"), max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Tuning:\n",
    "* Prompt tuning adds task-specific prompts to the input, and these prompt parameters are updated independently of the pretrained model parameters which are frozen.\n",
    "* Effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptEmbedding, PromptTuningConfig\n",
    "\n",
    "config = PromptTuningConfig(\n",
    "    peft_type=\"PROMPT_TUNING\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    num_virtual_tokens=20,\n",
    "    token_dim=768, # The hidden embedding dimension of the base transformer model.\n",
    "    num_transformer_submodules=1,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n",
    "    tokenizer_name_or_path=\"t5-base\",\n",
    ")\n",
    "\n",
    "# t5_model.shared is the word embeddings of the base model\n",
    "prompt_embedding = PromptEmbedding(config, t5_model.shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix tuning\n",
    "* Prefix tuning prefixes a series of task-specific vectors to the input sequence that can be learned while keeping the pretrained model frozen. \n",
    "* The prefix parameters are inserted in all of the model layers.\n",
    "* A lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). \n",
    "* Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PrefixEncoder, PrefixTuningConfig\n",
    "\n",
    "config = PrefixTuningConfig(\n",
    "    peft_type=\"PREFIX_TUNING\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    num_virtual_tokens=20,\n",
    "    token_dim=768,\n",
    "    num_transformer_submodules=1,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    encoder_hidden_size=768,\n",
    ")\n",
    "\n",
    "prefix_encoder = PrefixEncoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-tuning\n",
    "* P-tuning adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. \n",
    "* The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance.\n",
    "* employs trainable continuous prompt embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PromptEncoder, PromptEncoderConfig\n",
    "\n",
    "config = PromptEncoderConfig(\n",
    "    peft_type=\"P_TUNING\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    num_virtual_tokens=20,\n",
    "    token_dim=768,\n",
    "    num_transformer_submodules=1,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    encoder_reparameterization_type=\"MLP\",\n",
    "    encoder_hidden_size=768,\n",
    ")\n",
    "\n",
    "prompt_encoder = PromptEncoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA (Low Rank Adapter)\n",
    "* By default, PEFT initializes LoRA weights with Kaiming-uniform for weight A and zeros for weight B resulting in an identity transform .\n",
    "* init_lora_weights=\"gaussian\". As the name suggests, this results in initializing weight A with a Gaussian distribution. (Diffusion)\n",
    "* The LoRA architecture scales each adapter during every forward pass by a fixed scalar which is set at initialization and depends on the rank r. \n",
    "* **Rank-stabilized LoRA**: The scalar is given by lora_alpha/r in the original implementation, but rsLoRA uses lora_alpha/math.sqrt(r) which stabilizes the adapters and increases the performance potential from using a higher r.\n",
    "* **DoRA**: Splits weight update as magniture (a separate learnable parameter) and direction (LoRA). It needs to me merged before inference for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraModel, LoraConfig\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.01,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "lora_model = LoraModel(model, config, \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "rank = ...\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\"]\n",
    "config = LoraConfig(\n",
    "    r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "model = transformers.GPTJForCausalLM.from_pretrained(\n",
    "    \"kakaobrain/kogpt\",\n",
    "    revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=False,\n",
    "    device_map={\"\": rank},\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA\n",
    "* Use LoftQ while training using quantization.\n",
    "* The idea is that the LoRA weights are initialized such that the quantization error is minimized.\n",
    "* for LoftQ to work best, it is recommended to target as many layers with LoRA as possible, since those not targeted cannot have LoftQ applied. \n",
    "* This means that passing LoraConfig(..., target_modules=\"all-linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here\n",
    "loftq_config = LoftQConfig(loftq_bits=4, ...)           # set 4bit quantization\n",
    "lora_config = LoraConfig(target_modules=\"all-linear\", init_lora_weights=\"loftq\", loftq_config=loftq_config)\n",
    "peft_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Adapters\n",
    "* add_weighted_adapter() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id, adapter_name=\"sft\")\n",
    "\n",
    "model.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_adapter_name = \"sft-dpo\"\n",
    "\n",
    "model.add_weighted_adapter(\n",
    "    adapters=[\"sft\", \"dpo\"],\n",
    "    weights=[0.7, 0.3],\n",
    "    adapter_name=weighted_adapter_name,\n",
    "    combination_type=\"linear\"\n",
    ")\n",
    "model.set_adapter(weighted_adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "peft_model_id = \"alignment-handbook/zephyr-7b-sft-lora\"\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n",
    "# load different adapter\n",
    "model.load_adapter(\"alignment-handbook/zephyr-7b-dpo-lora\", adapter_name=\"dpo\")\n",
    "\n",
    "# set adapter as active\n",
    "model.set_adapter(\"dpo\")\n",
    "\n",
    "# unload adapter\n",
    "model.unload()\n",
    "\n",
    "# delete adapter\n",
    "model.delete_adapter(\"dpo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
